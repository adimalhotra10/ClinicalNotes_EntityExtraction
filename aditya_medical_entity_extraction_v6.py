# -*- coding: utf-8 -*-
"""Aditya_Medical_Entity_Extraction_V6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJMkF3K2atAv3p4cbp2Hnum5iB6kA4g_

# 1. Importing Data
"""

import scipy
import pandas as pd
import scispacy
import spacy
import re

data = pd.read_csv("ClinNotes.csv")
med_concepts = pd.read_csv("MedicalConcepts.csv")

#cleaning up
data['notes'] = data['notes'].apply(lambda x: re.sub('(\.,)', ". ", x))

sample=data.iloc[21][1]

sample

"""# 2. Entity Extraction

## 2.1 Entity Extraction using scispacy - en_ner_bc5cdr_md
"""

nlp = spacy.load("en_ner_bc5cdr_md")

doc = nlp(sample)

doc

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

from spacy import displacy
displacy.render(doc, style='ent', jupyter=True)

"""### Extracting drug and dosage information"""

from spacy.matcher import Matcher

pattern = [{'ENT_TYPE':'CHEMICAL'}, {'LIKE_NUM': True}, {'IS_ASCII': True}]
matcher = Matcher(nlp.vocab)
matcher.add("DRUG_DOSE", [pattern])

for transcription in data['notes']:
    doc = nlp(transcription)
    matches = matcher(doc)
    for match_id, start, end in matches:
        string_id = nlp.vocab.strings[match_id]  # get string representation
        span = doc[start:end]  # the matched span
        print(string_id, start, end, span.text)

"""## 2.2 Entity extraction with a different spacy model -  en_core_sci_sm"""

from spacy import displacy
from scispacy.abbreviation import AbbreviationDetector
from scispacy.umls_linking import UmlsEntityLinker

nlp = spacy.load("en_core_sci_sm") # just label entities.. Hence previous was better in that regard

doc = nlp(sample)

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

from spacy import displacy
displacy.render(next(doc.sents), style='dep', jupyter=True)

"""Showing how links are established through the model for a given sentence.

# 3. Linking entities to UMLS CUIs 

#### Difficult to do it here in Google Colab but giving it a shot. Ideal way of doing this is through entity extraction using Metamap UMLS service
"""

from scispacy.umls_linking import UmlsEntityLinker

linker = UmlsEntityLinker(resolve_abbreviations=True)

linker

nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})

doc =nlp(sample)

doc

entity = doc.ents[1]

print("Name: ", entity)

entity

"""Challenge: For each entity, mapping to more than one CUI with probabilities. 
 Naive approach: Choose for each entity the one with the max probability, but can be a problem if the highest probability is not the best one.

    
"""

for umls_ent in entity._.umls_ents:
    print(umls_ent)

linker.umls.cui_to_entity['C0028754']

#Find the entities and it's definition

entity = doc.ents
# Each entity is linked to UMLS with a score
# (currently just char-3gram matching).
for i in range(len(entity)):
    for umls_ent in entity[i]._.umls_ents:
        print(linker.umls.cui_to_entity[umls_ent[0]])

""" # 4. Working with en_ner_bc5cdr_md using Disease - Chemical labels"""

nlp = spacy.load("en_ner_bc5cdr_md")

doc=nlp(sample)

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

data['notes'][0]

"""# 5. UMLS CUI Tagging"""

linker = UmlsEntityLinker(resolve_abbreviations=True)

nlp.add_pipe("scispacy_linker", config={"resolve_abbreviations": True, "linker_name": "umls"})

doc = nlp(data['notes'][0])

for ent in doc.ents:
    print(ent)

# running the following code takes a lot of time in doc = nlp(iter) part.
CUI_codes=[]
for i in range(len(data['notes'])):
    CUI_codes.append([])
    doc=nlp(data['notes'][i]) # Takes a lot of time
    for ent in doc.ents:
        for umls_ent in ent._.umls_ents:
            CUI_codes[i].append((tuple((ent,umls_ent))))

"""# Entity atrial is linked to 5 CUI codes. Trying to pick up the cui with max probability"""

CUI_codes

# picking the one with max probability. Crashes in local. Moved back to google colab. 
## 99999999999 represents absence of a cui
CUI_codes=[]
for i in range(len(data['notes'])):
    print(i, flush = True)
    CUI_codes.append([])
    doc=nlp(data['notes'][i]) # Takes a lot of time
    for ent in doc.ents:
        if len(ent._.umls_ents) > 0:
            CUI_codes[i].append((tuple((ent,ent._.umls_ents[0]))))
        else:
            CUI_codes[i].append((tuple((ent,99999999999))))